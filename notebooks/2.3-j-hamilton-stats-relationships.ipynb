{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json, operator, os, re, requests, string, pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from pandas.io.json import json_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../data/processed/hamilton_data.json', 'r') as f:\n",
    "    haml = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basestats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "track#\n",
      "track\n",
      "act#\n",
      "lyrics\n"
     ]
    }
   ],
   "source": [
    "for key in haml[0]:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for song in haml:\n",
    "    song['act#'] = int(song['act#'])\n",
    "    song['track#'] = int(song['track#'])\n",
    "haml.sort(key=operator.itemgetter('act#','track#'))\n",
    "#for song in haml:\n",
    "#    print('Act:', song['act#'], '\\tTrack#:', song['track#'], '\\tTrack:', song['track'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "basestats = json_normalize(haml)[['act#', 'track#','track','lyrics']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "line_length, word_length = [], []\n",
    "for song in haml:    \n",
    "    line_length.append(len(song['lyrics']))\n",
    "    words_in_song = 0\n",
    "    for line in song['lyrics']:\n",
    "        words_in_song += len(line['tokenized'])\n",
    "    word_length.append(words_in_song)    \n",
    "basestats['#lines'] = line_length\n",
    "basestats['#words'] = word_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = requests.get('https://en.wikipedia.org/w/index.php?title=Hamilton_(album)&oldid=751292662')\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "tbl1, tbl2 = soup.find_all('table')[3], soup.find_all('table')[4]\n",
    "rows1, rows2 = tbl1.find_all('tr'), tbl2.find_all('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "song_length = []\n",
    "for row in rows1[:-1]:  # Exclude last line, cause it's just a summary\n",
    "    if len(row.find_all('td'))==0:\n",
    "        continue  # Exclude empty rows\n",
    "    row_compl = row.find_all('td')  # Find all cells\n",
    "    song_length.append(row_compl[3].text)  # Get row giving the length of song\n",
    "for row in rows2[:-1]:  # Exclude last line, cause it's just a summary\n",
    "    if len(row.find_all('td'))==0:\n",
    "        continue  # Exclude empty rows\n",
    "    row_compl = row.find_all('td')  # Find all cells\n",
    "    song_length.append(row_compl[3].text)  # Get row giving the length of song\n",
    "basestats['len_seconds'] = [int(m[0])*60+int(m[2:4]) for m in song_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_speakers = []\n",
    "speakers_song_ls = []\n",
    "for song in haml:\n",
    "    speakers_song = []\n",
    "    for line in song['lyrics']:\n",
    "        all_speakers.append(line['speakers'])\n",
    "        speakers_song.append(line['speakers'])\n",
    "    #speakers_song_ls.append(set(speakers_song))\n",
    "    speakers_song_ls.append(set([item for sublist in speakers_song for item in sublist]))\n",
    "    #print(set([item for sublist in speakers_song for item in sublist]))\n",
    "all_speakers = set([item for sublist in all_speakers for item in sublist])\n",
    "basestats['speakers'] = speakers_song_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_speakers = []\n",
    "for spkls in speakers_song_ls:\n",
    "    num_speakers.append(len(spkls))\n",
    "basestats['#speakers'] = num_speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "basestats['#words/sec'] = basestats['#words']/basestats['len_seconds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "basestats.drop('lyrics', axis=1, inplace=False).to_csv('../data/processed/basestats.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>act#</th>\n",
       "      <th>track#</th>\n",
       "      <th>track</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>#lines</th>\n",
       "      <th>#words</th>\n",
       "      <th>len_seconds</th>\n",
       "      <th>speakers</th>\n",
       "      <th>#speakers</th>\n",
       "      <th>#words/sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Alexander Hamilton</td>\n",
       "      <td>[{'tokenized': ['How', 'does', 'a', 'bastard',...</td>\n",
       "      <td>93</td>\n",
       "      <td>615</td>\n",
       "      <td>236</td>\n",
       "      <td>{BURR, MADISON, MULLIGAN, MARIA, LAFAYETTE, PE...</td>\n",
       "      <td>17</td>\n",
       "      <td>2.605932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Aaron Burr, Sir</td>\n",
       "      <td>[{'tokenized': ['1776', 'New', 'York', 'City']...</td>\n",
       "      <td>66</td>\n",
       "      <td>456</td>\n",
       "      <td>156</td>\n",
       "      <td>{BURR, MULLIGAN, LAFAYETTE, LAURENS, COMPANY, ...</td>\n",
       "      <td>6</td>\n",
       "      <td>2.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>My Shot</td>\n",
       "      <td>[{'tokenized': ['I', 'am', 'not', 'throwing', ...</td>\n",
       "      <td>192</td>\n",
       "      <td>1198</td>\n",
       "      <td>333</td>\n",
       "      <td>{BURR, MULLIGAN, LAFAYETTE, LAURENS, ENSEMBLE,...</td>\n",
       "      <td>7</td>\n",
       "      <td>3.597598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>The Story Of Tonight</td>\n",
       "      <td>[{'tokenized': ['I', 'may', 'not', 'live', 'to...</td>\n",
       "      <td>32</td>\n",
       "      <td>200</td>\n",
       "      <td>91</td>\n",
       "      <td>{MULLIGAN, LAFAYETTE, FULL ENSEMBLE, LAURENS, ...</td>\n",
       "      <td>6</td>\n",
       "      <td>2.197802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>The Schuyler Sisters</td>\n",
       "      <td>[{'tokenized': ['There', 's', 'nothing', 'rich...</td>\n",
       "      <td>122</td>\n",
       "      <td>613</td>\n",
       "      <td>186</td>\n",
       "      <td>{BURR, ALL MEN, PEGGY, ALL WOMEN, ELIZA, MALE ...</td>\n",
       "      <td>13</td>\n",
       "      <td>3.295699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>Farmer Refuted</td>\n",
       "      <td>[{'tokenized': ['Hear', 'ye', 'hear', 'ye', 'M...</td>\n",
       "      <td>56</td>\n",
       "      <td>300</td>\n",
       "      <td>112</td>\n",
       "      <td>{BURR, MULLIGAN, SEABURY, FULL COMPANY, ENSEMB...</td>\n",
       "      <td>7</td>\n",
       "      <td>2.678571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>You'll Be Back</td>\n",
       "      <td>[{'tokenized': ['You', 'say'], 'normalized': [...</td>\n",
       "      <td>44</td>\n",
       "      <td>349</td>\n",
       "      <td>208</td>\n",
       "      <td>{FULL ENSEMBLE, KING GEORGE}</td>\n",
       "      <td>2</td>\n",
       "      <td>1.677885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>Right Hand Man</td>\n",
       "      <td>[{'tokenized': ['British', 'Admiral', 'Howe', ...</td>\n",
       "      <td>180</td>\n",
       "      <td>988</td>\n",
       "      <td>321</td>\n",
       "      <td>{ENSEMBLE 2, BURR, MULLIGAN, LAFAYETTE, ENSEMB...</td>\n",
       "      <td>15</td>\n",
       "      <td>3.077882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>A Winter's Ball</td>\n",
       "      <td>[{'tokenized': ['How', 'does', 'the', 'bastard...</td>\n",
       "      <td>26</td>\n",
       "      <td>137</td>\n",
       "      <td>69</td>\n",
       "      <td>{LAURENS, BURR, FULL COMPANY, HAMILTON, ALL MEN}</td>\n",
       "      <td>5</td>\n",
       "      <td>1.985507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>Helpless</td>\n",
       "      <td>[{'tokenized': ['Hey', 'hey', 'hey', 'hey'], '...</td>\n",
       "      <td>150</td>\n",
       "      <td>803</td>\n",
       "      <td>249</td>\n",
       "      <td>{BURR, ALL WOMEN, ELIZA, WOMEN, LAURENS, ANGEL...</td>\n",
       "      <td>7</td>\n",
       "      <td>3.224900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>Satisfied</td>\n",
       "      <td>[{'tokenized': ['Alright', 'alright', 'That', ...</td>\n",
       "      <td>182</td>\n",
       "      <td>901</td>\n",
       "      <td>329</td>\n",
       "      <td>{ALL WOMEN, ALL MEN, ELIZA, RECORDED SAMPLES, ...</td>\n",
       "      <td>11</td>\n",
       "      <td>2.738602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>The Story Of Tonight (Reprise)</td>\n",
       "      <td>[{'tokenized': ['I', 'may', 'not', 'live', 'to...</td>\n",
       "      <td>56</td>\n",
       "      <td>323</td>\n",
       "      <td>115</td>\n",
       "      <td>{LAURENS, BURR, MULLIGAN, HAMILTON, LAFAYETTE}</td>\n",
       "      <td>5</td>\n",
       "      <td>2.808696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>Wait For It</td>\n",
       "      <td>[{'tokenized': ['Theodosia', 'writes', 'me', '...</td>\n",
       "      <td>126</td>\n",
       "      <td>550</td>\n",
       "      <td>193</td>\n",
       "      <td>{WOMEN, ENSEMBLE, BURR, MEN, COMPANY}</td>\n",
       "      <td>5</td>\n",
       "      <td>2.849741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>Stay Alive</td>\n",
       "      <td>[{'tokenized': ['Stay', 'alive'], 'normalized'...</td>\n",
       "      <td>76</td>\n",
       "      <td>434</td>\n",
       "      <td>159</td>\n",
       "      <td>{MULLIGAN, ENSEMBLE WOMEN, LAFAYETTE, LEE, WAS...</td>\n",
       "      <td>10</td>\n",
       "      <td>2.729560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>Ten Duel Commandments</td>\n",
       "      <td>[{'tokenized': ['One', 'two', 'three', 'four']...</td>\n",
       "      <td>50</td>\n",
       "      <td>291</td>\n",
       "      <td>106</td>\n",
       "      <td>{BURR, LEE, FULL COMPANY, LAURENS, COMPANY, ME...</td>\n",
       "      <td>7</td>\n",
       "      <td>2.745283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>Meet Me Inside</td>\n",
       "      <td>[{'tokenized': ['Lee', 'do', 'you', 'yield'], ...</td>\n",
       "      <td>48</td>\n",
       "      <td>280</td>\n",
       "      <td>83</td>\n",
       "      <td>{WASHINGTON, LAURENS, COMPANY, BURR, HAMILTON}</td>\n",
       "      <td>5</td>\n",
       "      <td>3.373494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>That Would Be Enough</td>\n",
       "      <td>[{'tokenized': ['Look', 'around', 'look', 'aro...</td>\n",
       "      <td>48</td>\n",
       "      <td>285</td>\n",
       "      <td>178</td>\n",
       "      <td>{ELIZA, HAMILTON}</td>\n",
       "      <td>2</td>\n",
       "      <td>1.601124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>Guns And Ships</td>\n",
       "      <td>[{'tokenized': ['How', 'does', 'a', 'ragtag', ...</td>\n",
       "      <td>64</td>\n",
       "      <td>355</td>\n",
       "      <td>127</td>\n",
       "      <td>{BURR, ENSEMBLE, LAFAYETTE, WOMEN, WASHINGTON,...</td>\n",
       "      <td>7</td>\n",
       "      <td>2.795276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>History Has Its Eyes On You</td>\n",
       "      <td>[{'tokenized': ['I', 'was', 'younger', 'than',...</td>\n",
       "      <td>30</td>\n",
       "      <td>129</td>\n",
       "      <td>97</td>\n",
       "      <td>{MULLIGAN, ENSEMBLE, FULL COMPANY, LAURENS, WA...</td>\n",
       "      <td>8</td>\n",
       "      <td>1.329897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>Yorktown (The World Turned Upside Down)</td>\n",
       "      <td>[{'tokenized': ['The', 'battle', 'of', 'Yorkto...</td>\n",
       "      <td>95</td>\n",
       "      <td>617</td>\n",
       "      <td>242</td>\n",
       "      <td>{MULLIGAN, LAFAYETTE, ALL MEN, WASHINGTON, FUL...</td>\n",
       "      <td>9</td>\n",
       "      <td>2.549587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>What Comes Next</td>\n",
       "      <td>[{'tokenized': ['They', 'say'], 'normalized': ...</td>\n",
       "      <td>23</td>\n",
       "      <td>135</td>\n",
       "      <td>99</td>\n",
       "      <td>{KING GEORGE}</td>\n",
       "      <td>1</td>\n",
       "      <td>1.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>Dear Theodosia</td>\n",
       "      <td>[{'tokenized': ['Dear', 'Theodosia', 'what', '...</td>\n",
       "      <td>39</td>\n",
       "      <td>284</td>\n",
       "      <td>184</td>\n",
       "      <td>{BURR, HAMILTON}</td>\n",
       "      <td>2</td>\n",
       "      <td>1.543478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>Non-Stop</td>\n",
       "      <td>[{'tokenized': ['After', 'the', 'war', 'I', 'w...</td>\n",
       "      <td>230</td>\n",
       "      <td>1327</td>\n",
       "      <td>385</td>\n",
       "      <td>{ENSEMBLE MAN, BURR, MULLIGAN, ALL WOMEN, LAFA...</td>\n",
       "      <td>15</td>\n",
       "      <td>3.446753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>What'd I Miss</td>\n",
       "      <td>[{'tokenized': ['Seventeen', 'Se', 'se', 'seve...</td>\n",
       "      <td>94</td>\n",
       "      <td>512</td>\n",
       "      <td>236</td>\n",
       "      <td>{BURR, MADISON, WASHINGTON, JEFFERSON, ENSEMBL...</td>\n",
       "      <td>7</td>\n",
       "      <td>2.169492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Cabinet Battle #1</td>\n",
       "      <td>[{'tokenized': ['Ladies', 'and', 'gentlemen', ...</td>\n",
       "      <td>78</td>\n",
       "      <td>623</td>\n",
       "      <td>215</td>\n",
       "      <td>{JEFFERSON, CROWD, WASHINGTON, MADISON, HAMILTON}</td>\n",
       "      <td>5</td>\n",
       "      <td>2.897674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Take a Break</td>\n",
       "      <td>[{'tokenized': ['Un', 'deux', 'trois', 'quatre...</td>\n",
       "      <td>137</td>\n",
       "      <td>737</td>\n",
       "      <td>286</td>\n",
       "      <td>{ANGELICA, ELIZA, PHILIP, HAMILTON}</td>\n",
       "      <td>4</td>\n",
       "      <td>2.576923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Say No to This</td>\n",
       "      <td>[{'tokenized': ['There’s', 'nothing', 'like', ...</td>\n",
       "      <td>133</td>\n",
       "      <td>664</td>\n",
       "      <td>242</td>\n",
       "      <td>{ENSEMBLE, BURR, MADISON, MARIA, HAMILTON}</td>\n",
       "      <td>5</td>\n",
       "      <td>2.743802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>The Room Where It Happens</td>\n",
       "      <td>[{'tokenized': ['Ah', 'Mister', 'Secretary'], ...</td>\n",
       "      <td>209</td>\n",
       "      <td>1056</td>\n",
       "      <td>318</td>\n",
       "      <td>{BURR, MADISON, WASHINGTON, JEFFERSON, ENSEMBL...</td>\n",
       "      <td>7</td>\n",
       "      <td>3.320755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>Schuyler Defeated</td>\n",
       "      <td>[{'tokenized': ['Look'], 'normalized': ['look'...</td>\n",
       "      <td>31</td>\n",
       "      <td>180</td>\n",
       "      <td>63</td>\n",
       "      <td>{BURR, ELIZA, PHILIP, HAMILTON}</td>\n",
       "      <td>4</td>\n",
       "      <td>2.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>Cabinet Battle #2</td>\n",
       "      <td>[{'tokenized': ['The', 'issue', 'on', 'the', '...</td>\n",
       "      <td>62</td>\n",
       "      <td>448</td>\n",
       "      <td>142</td>\n",
       "      <td>{JEFFERSON, WASHINGTON, MADISON, ENSEMBLE, HAM...</td>\n",
       "      <td>5</td>\n",
       "      <td>3.154930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>Washington On Your Side</td>\n",
       "      <td>[{'tokenized': ['It', 'must', 'be', 'nice', 'i...</td>\n",
       "      <td>68</td>\n",
       "      <td>457</td>\n",
       "      <td>181</td>\n",
       "      <td>{JEFFERSON, ENSEMBLE, BURR, MADISON}</td>\n",
       "      <td>4</td>\n",
       "      <td>2.524862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>One Last Time</td>\n",
       "      <td>[{'tokenized': ['Mr', 'President', 'you', 'ask...</td>\n",
       "      <td>99</td>\n",
       "      <td>675</td>\n",
       "      <td>296</td>\n",
       "      <td>{WASHINGTON, COMPANY, HAMILTON, ALL WOMEN}</td>\n",
       "      <td>4</td>\n",
       "      <td>2.280405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>I Know Him</td>\n",
       "      <td>[{'tokenized': ['They', 'say'], 'normalized': ...</td>\n",
       "      <td>28</td>\n",
       "      <td>143</td>\n",
       "      <td>97</td>\n",
       "      <td>{KING GEORGE}</td>\n",
       "      <td>1</td>\n",
       "      <td>1.474227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>The Adams Administration</td>\n",
       "      <td>[{'tokenized': ['How', 'does', 'Hamilton', 'th...</td>\n",
       "      <td>17</td>\n",
       "      <td>129</td>\n",
       "      <td>54</td>\n",
       "      <td>{JEFFERSON, HAMILTON, BURR, MADISON, COMPANY}</td>\n",
       "      <td>5</td>\n",
       "      <td>2.388889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>We Know</td>\n",
       "      <td>[{'tokenized': ['Mr', 'Vice', 'President'], 'n...</td>\n",
       "      <td>56</td>\n",
       "      <td>383</td>\n",
       "      <td>142</td>\n",
       "      <td>{JEFFERSON, BURR, MADISON, HAMILTON}</td>\n",
       "      <td>4</td>\n",
       "      <td>2.697183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>Hurricane</td>\n",
       "      <td>[{'tokenized': ['In', 'the', 'eye', 'of', 'a',...</td>\n",
       "      <td>46</td>\n",
       "      <td>289</td>\n",
       "      <td>143</td>\n",
       "      <td>{BURR, ENSEMBLE, MARIA, ELIZA, ANGELICA, WASHI...</td>\n",
       "      <td>8</td>\n",
       "      <td>2.020979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>The Reynolds Pamphlet</td>\n",
       "      <td>[{'tokenized': ['The', 'Reynolds', 'Pamphlet']...</td>\n",
       "      <td>82</td>\n",
       "      <td>410</td>\n",
       "      <td>128</td>\n",
       "      <td>{ENSEMBLE MEN, BURR, MADISON, ENSEMBLE WOMEN, ...</td>\n",
       "      <td>11</td>\n",
       "      <td>3.203125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>Burn</td>\n",
       "      <td>[{'tokenized': ['I', 'saved', 'every', 'letter...</td>\n",
       "      <td>54</td>\n",
       "      <td>297</td>\n",
       "      <td>225</td>\n",
       "      <td>{ELIZA}</td>\n",
       "      <td>1</td>\n",
       "      <td>1.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>Blow Us All Away</td>\n",
       "      <td>[{'tokenized': ['Meet', 'the', 'latest', 'grad...</td>\n",
       "      <td>81</td>\n",
       "      <td>546</td>\n",
       "      <td>173</td>\n",
       "      <td>{MARTHA, PHILIP, BOTH, MALE ENSEMBLE, GEORGE, ...</td>\n",
       "      <td>10</td>\n",
       "      <td>3.156069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>Stay Alive (Reprise)</td>\n",
       "      <td>[{'tokenized': ['Stay', 'alive'], 'normalized'...</td>\n",
       "      <td>57</td>\n",
       "      <td>253</td>\n",
       "      <td>111</td>\n",
       "      <td>{ENSEMBLE MEN, ENSEMBLE WOMEN, ELIZA, PHILIP, ...</td>\n",
       "      <td>6</td>\n",
       "      <td>2.279279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>It's Quiet Uptown</td>\n",
       "      <td>[{'tokenized': ['There', 'are', 'moments', 'th...</td>\n",
       "      <td>62</td>\n",
       "      <td>393</td>\n",
       "      <td>270</td>\n",
       "      <td>{ALL MEN, ELIZA, WOMEN, ANGELICA, ENSEMBLE, CO...</td>\n",
       "      <td>7</td>\n",
       "      <td>1.455556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>The Election of 1800</td>\n",
       "      <td>[{'tokenized': ['The', 'election', 'of', '1800...</td>\n",
       "      <td>144</td>\n",
       "      <td>679</td>\n",
       "      <td>237</td>\n",
       "      <td>{MALE VOTER, TWO WOMEN, BURR, MADISON, FEMALE ...</td>\n",
       "      <td>18</td>\n",
       "      <td>2.864979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>Your Obedient Servant</td>\n",
       "      <td>[{'tokenized': ['How', 'does', 'Hamilton'], 'n...</td>\n",
       "      <td>61</td>\n",
       "      <td>326</td>\n",
       "      <td>150</td>\n",
       "      <td>{HAMILTON, BURR, COMPANY}</td>\n",
       "      <td>3</td>\n",
       "      <td>2.173333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>Best of Wives and Best of Women</td>\n",
       "      <td>[{'tokenized': ['Alexander', 'come', 'back', '...</td>\n",
       "      <td>12</td>\n",
       "      <td>75</td>\n",
       "      <td>47</td>\n",
       "      <td>{ELIZA, HAMILTON}</td>\n",
       "      <td>2</td>\n",
       "      <td>1.595745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>The World Was Wide Enough</td>\n",
       "      <td>[{'tokenized': ['One', 'two', 'three', 'four']...</td>\n",
       "      <td>116</td>\n",
       "      <td>680</td>\n",
       "      <td>302</td>\n",
       "      <td>{MALE COMPANY, ENSEMBLE MEN, BURR, PHILIP, FUL...</td>\n",
       "      <td>8</td>\n",
       "      <td>2.251656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>Who Lives, Who Dies, Who Tells Your Story</td>\n",
       "      <td>[{'tokenized': ['Let', 'me', 'tell', 'you', 'w...</td>\n",
       "      <td>88</td>\n",
       "      <td>424</td>\n",
       "      <td>217</td>\n",
       "      <td>{BURR, MADISON, MULLIGAN, LAFAYETTE, ELIZA, WO...</td>\n",
       "      <td>13</td>\n",
       "      <td>1.953917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    act#  track#                                      track  \\\n",
       "0      1       1                         Alexander Hamilton   \n",
       "1      1       2                            Aaron Burr, Sir   \n",
       "2      1       3                                    My Shot   \n",
       "3      1       4                       The Story Of Tonight   \n",
       "4      1       5                       The Schuyler Sisters   \n",
       "5      1       6                             Farmer Refuted   \n",
       "6      1       7                             You'll Be Back   \n",
       "7      1       8                             Right Hand Man   \n",
       "8      1       9                            A Winter's Ball   \n",
       "9      1      10                                   Helpless   \n",
       "10     1      11                                  Satisfied   \n",
       "11     1      12             The Story Of Tonight (Reprise)   \n",
       "12     1      13                                Wait For It   \n",
       "13     1      14                                 Stay Alive   \n",
       "14     1      15                      Ten Duel Commandments   \n",
       "15     1      16                             Meet Me Inside   \n",
       "16     1      17                       That Would Be Enough   \n",
       "17     1      18                             Guns And Ships   \n",
       "18     1      19                History Has Its Eyes On You   \n",
       "19     1      20    Yorktown (The World Turned Upside Down)   \n",
       "20     1      21                            What Comes Next   \n",
       "21     1      22                             Dear Theodosia   \n",
       "22     1      23                                   Non-Stop   \n",
       "23     2       1                              What'd I Miss   \n",
       "24     2       2                          Cabinet Battle #1   \n",
       "25     2       3                               Take a Break   \n",
       "26     2       4                             Say No to This   \n",
       "27     2       5                  The Room Where It Happens   \n",
       "28     2       6                          Schuyler Defeated   \n",
       "29     2       7                          Cabinet Battle #2   \n",
       "30     2       8                    Washington On Your Side   \n",
       "31     2       9                              One Last Time   \n",
       "32     2      10                                 I Know Him   \n",
       "33     2      11                   The Adams Administration   \n",
       "34     2      12                                    We Know   \n",
       "35     2      13                                  Hurricane   \n",
       "36     2      14                      The Reynolds Pamphlet   \n",
       "37     2      15                                       Burn   \n",
       "38     2      16                           Blow Us All Away   \n",
       "39     2      17                       Stay Alive (Reprise)   \n",
       "40     2      18                          It's Quiet Uptown   \n",
       "41     2      19                       The Election of 1800   \n",
       "42     2      20                      Your Obedient Servant   \n",
       "43     2      21            Best of Wives and Best of Women   \n",
       "44     2      22                  The World Was Wide Enough   \n",
       "45     2      23  Who Lives, Who Dies, Who Tells Your Story   \n",
       "\n",
       "                                               lyrics  #lines  #words  \\\n",
       "0   [{'tokenized': ['How', 'does', 'a', 'bastard',...      93     615   \n",
       "1   [{'tokenized': ['1776', 'New', 'York', 'City']...      66     456   \n",
       "2   [{'tokenized': ['I', 'am', 'not', 'throwing', ...     192    1198   \n",
       "3   [{'tokenized': ['I', 'may', 'not', 'live', 'to...      32     200   \n",
       "4   [{'tokenized': ['There', 's', 'nothing', 'rich...     122     613   \n",
       "5   [{'tokenized': ['Hear', 'ye', 'hear', 'ye', 'M...      56     300   \n",
       "6   [{'tokenized': ['You', 'say'], 'normalized': [...      44     349   \n",
       "7   [{'tokenized': ['British', 'Admiral', 'Howe', ...     180     988   \n",
       "8   [{'tokenized': ['How', 'does', 'the', 'bastard...      26     137   \n",
       "9   [{'tokenized': ['Hey', 'hey', 'hey', 'hey'], '...     150     803   \n",
       "10  [{'tokenized': ['Alright', 'alright', 'That', ...     182     901   \n",
       "11  [{'tokenized': ['I', 'may', 'not', 'live', 'to...      56     323   \n",
       "12  [{'tokenized': ['Theodosia', 'writes', 'me', '...     126     550   \n",
       "13  [{'tokenized': ['Stay', 'alive'], 'normalized'...      76     434   \n",
       "14  [{'tokenized': ['One', 'two', 'three', 'four']...      50     291   \n",
       "15  [{'tokenized': ['Lee', 'do', 'you', 'yield'], ...      48     280   \n",
       "16  [{'tokenized': ['Look', 'around', 'look', 'aro...      48     285   \n",
       "17  [{'tokenized': ['How', 'does', 'a', 'ragtag', ...      64     355   \n",
       "18  [{'tokenized': ['I', 'was', 'younger', 'than',...      30     129   \n",
       "19  [{'tokenized': ['The', 'battle', 'of', 'Yorkto...      95     617   \n",
       "20  [{'tokenized': ['They', 'say'], 'normalized': ...      23     135   \n",
       "21  [{'tokenized': ['Dear', 'Theodosia', 'what', '...      39     284   \n",
       "22  [{'tokenized': ['After', 'the', 'war', 'I', 'w...     230    1327   \n",
       "23  [{'tokenized': ['Seventeen', 'Se', 'se', 'seve...      94     512   \n",
       "24  [{'tokenized': ['Ladies', 'and', 'gentlemen', ...      78     623   \n",
       "25  [{'tokenized': ['Un', 'deux', 'trois', 'quatre...     137     737   \n",
       "26  [{'tokenized': ['There’s', 'nothing', 'like', ...     133     664   \n",
       "27  [{'tokenized': ['Ah', 'Mister', 'Secretary'], ...     209    1056   \n",
       "28  [{'tokenized': ['Look'], 'normalized': ['look'...      31     180   \n",
       "29  [{'tokenized': ['The', 'issue', 'on', 'the', '...      62     448   \n",
       "30  [{'tokenized': ['It', 'must', 'be', 'nice', 'i...      68     457   \n",
       "31  [{'tokenized': ['Mr', 'President', 'you', 'ask...      99     675   \n",
       "32  [{'tokenized': ['They', 'say'], 'normalized': ...      28     143   \n",
       "33  [{'tokenized': ['How', 'does', 'Hamilton', 'th...      17     129   \n",
       "34  [{'tokenized': ['Mr', 'Vice', 'President'], 'n...      56     383   \n",
       "35  [{'tokenized': ['In', 'the', 'eye', 'of', 'a',...      46     289   \n",
       "36  [{'tokenized': ['The', 'Reynolds', 'Pamphlet']...      82     410   \n",
       "37  [{'tokenized': ['I', 'saved', 'every', 'letter...      54     297   \n",
       "38  [{'tokenized': ['Meet', 'the', 'latest', 'grad...      81     546   \n",
       "39  [{'tokenized': ['Stay', 'alive'], 'normalized'...      57     253   \n",
       "40  [{'tokenized': ['There', 'are', 'moments', 'th...      62     393   \n",
       "41  [{'tokenized': ['The', 'election', 'of', '1800...     144     679   \n",
       "42  [{'tokenized': ['How', 'does', 'Hamilton'], 'n...      61     326   \n",
       "43  [{'tokenized': ['Alexander', 'come', 'back', '...      12      75   \n",
       "44  [{'tokenized': ['One', 'two', 'three', 'four']...     116     680   \n",
       "45  [{'tokenized': ['Let', 'me', 'tell', 'you', 'w...      88     424   \n",
       "\n",
       "    len_seconds                                           speakers  #speakers  \\\n",
       "0           236  {BURR, MADISON, MULLIGAN, MARIA, LAFAYETTE, PE...         17   \n",
       "1           156  {BURR, MULLIGAN, LAFAYETTE, LAURENS, COMPANY, ...          6   \n",
       "2           333  {BURR, MULLIGAN, LAFAYETTE, LAURENS, ENSEMBLE,...          7   \n",
       "3            91  {MULLIGAN, LAFAYETTE, FULL ENSEMBLE, LAURENS, ...          6   \n",
       "4           186  {BURR, ALL MEN, PEGGY, ALL WOMEN, ELIZA, MALE ...         13   \n",
       "5           112  {BURR, MULLIGAN, SEABURY, FULL COMPANY, ENSEMB...          7   \n",
       "6           208                       {FULL ENSEMBLE, KING GEORGE}          2   \n",
       "7           321  {ENSEMBLE 2, BURR, MULLIGAN, LAFAYETTE, ENSEMB...         15   \n",
       "8            69   {LAURENS, BURR, FULL COMPANY, HAMILTON, ALL MEN}          5   \n",
       "9           249  {BURR, ALL WOMEN, ELIZA, WOMEN, LAURENS, ANGEL...          7   \n",
       "10          329  {ALL WOMEN, ALL MEN, ELIZA, RECORDED SAMPLES, ...         11   \n",
       "11          115     {LAURENS, BURR, MULLIGAN, HAMILTON, LAFAYETTE}          5   \n",
       "12          193              {WOMEN, ENSEMBLE, BURR, MEN, COMPANY}          5   \n",
       "13          159  {MULLIGAN, ENSEMBLE WOMEN, LAFAYETTE, LEE, WAS...         10   \n",
       "14          106  {BURR, LEE, FULL COMPANY, LAURENS, COMPANY, ME...          7   \n",
       "15           83     {WASHINGTON, LAURENS, COMPANY, BURR, HAMILTON}          5   \n",
       "16          178                                  {ELIZA, HAMILTON}          2   \n",
       "17          127  {BURR, ENSEMBLE, LAFAYETTE, WOMEN, WASHINGTON,...          7   \n",
       "18           97  {MULLIGAN, ENSEMBLE, FULL COMPANY, LAURENS, WA...          8   \n",
       "19          242  {MULLIGAN, LAFAYETTE, ALL MEN, WASHINGTON, FUL...          9   \n",
       "20           99                                      {KING GEORGE}          1   \n",
       "21          184                                   {BURR, HAMILTON}          2   \n",
       "22          385  {ENSEMBLE MAN, BURR, MULLIGAN, ALL WOMEN, LAFA...         15   \n",
       "23          236  {BURR, MADISON, WASHINGTON, JEFFERSON, ENSEMBL...          7   \n",
       "24          215  {JEFFERSON, CROWD, WASHINGTON, MADISON, HAMILTON}          5   \n",
       "25          286                {ANGELICA, ELIZA, PHILIP, HAMILTON}          4   \n",
       "26          242         {ENSEMBLE, BURR, MADISON, MARIA, HAMILTON}          5   \n",
       "27          318  {BURR, MADISON, WASHINGTON, JEFFERSON, ENSEMBL...          7   \n",
       "28           63                    {BURR, ELIZA, PHILIP, HAMILTON}          4   \n",
       "29          142  {JEFFERSON, WASHINGTON, MADISON, ENSEMBLE, HAM...          5   \n",
       "30          181               {JEFFERSON, ENSEMBLE, BURR, MADISON}          4   \n",
       "31          296         {WASHINGTON, COMPANY, HAMILTON, ALL WOMEN}          4   \n",
       "32           97                                      {KING GEORGE}          1   \n",
       "33           54      {JEFFERSON, HAMILTON, BURR, MADISON, COMPANY}          5   \n",
       "34          142               {JEFFERSON, BURR, MADISON, HAMILTON}          4   \n",
       "35          143  {BURR, ENSEMBLE, MARIA, ELIZA, ANGELICA, WASHI...          8   \n",
       "36          128  {ENSEMBLE MEN, BURR, MADISON, ENSEMBLE WOMEN, ...         11   \n",
       "37          225                                            {ELIZA}          1   \n",
       "38          173  {MARTHA, PHILIP, BOTH, MALE ENSEMBLE, GEORGE, ...         10   \n",
       "39          111  {ENSEMBLE MEN, ENSEMBLE WOMEN, ELIZA, PHILIP, ...          6   \n",
       "40          270  {ALL MEN, ELIZA, WOMEN, ANGELICA, ENSEMBLE, CO...          7   \n",
       "41          237  {MALE VOTER, TWO WOMEN, BURR, MADISON, FEMALE ...         18   \n",
       "42          150                          {HAMILTON, BURR, COMPANY}          3   \n",
       "43           47                                  {ELIZA, HAMILTON}          2   \n",
       "44          302  {MALE COMPANY, ENSEMBLE MEN, BURR, PHILIP, FUL...          8   \n",
       "45          217  {BURR, MADISON, MULLIGAN, LAFAYETTE, ELIZA, WO...         13   \n",
       "\n",
       "    #words/sec  \n",
       "0     2.605932  \n",
       "1     2.923077  \n",
       "2     3.597598  \n",
       "3     2.197802  \n",
       "4     3.295699  \n",
       "5     2.678571  \n",
       "6     1.677885  \n",
       "7     3.077882  \n",
       "8     1.985507  \n",
       "9     3.224900  \n",
       "10    2.738602  \n",
       "11    2.808696  \n",
       "12    2.849741  \n",
       "13    2.729560  \n",
       "14    2.745283  \n",
       "15    3.373494  \n",
       "16    1.601124  \n",
       "17    2.795276  \n",
       "18    1.329897  \n",
       "19    2.549587  \n",
       "20    1.363636  \n",
       "21    1.543478  \n",
       "22    3.446753  \n",
       "23    2.169492  \n",
       "24    2.897674  \n",
       "25    2.576923  \n",
       "26    2.743802  \n",
       "27    3.320755  \n",
       "28    2.857143  \n",
       "29    3.154930  \n",
       "30    2.524862  \n",
       "31    2.280405  \n",
       "32    1.474227  \n",
       "33    2.388889  \n",
       "34    2.697183  \n",
       "35    2.020979  \n",
       "36    3.203125  \n",
       "37    1.320000  \n",
       "38    3.156069  \n",
       "39    2.279279  \n",
       "40    1.455556  \n",
       "41    2.864979  \n",
       "42    2.173333  \n",
       "43    1.595745  \n",
       "44    2.251656  \n",
       "45    1.953917  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basestats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count words per cast member"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = requests.get('https://en.wikipedia.org/w/index.php?title=Hamilton_(musical)&oldid=751273119')\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "cast = soup.find_all('table')[4]\n",
    "castr = cast.find_all('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "main_cast = []\n",
    "for row in castr:\n",
    "    if len(row.find_all('th'))==0:\n",
    "        continue  # Exclude empty rows\n",
    "    row_compl = row.find_all('th')  # Find all cells\n",
    "    cur_speaker_row = row_compl[0].text.split(' / ')\n",
    "    for whole_name in cur_speaker_row:\n",
    "        whole_name_list = whole_name.split(' ')\n",
    "        if 'Character' in whole_name_list:\n",
    "            continue\n",
    "        # Women, children, and kings are referred to by first name\n",
    "        elif 'Schuyler' in whole_name_list:\n",
    "            main_cast.append(whole_name_list[0].upper())\n",
    "        elif 'Reynolds' in whole_name_list:          \n",
    "            main_cast.append(whole_name_list[0].upper())\n",
    "        elif 'Philip' in whole_name_list:          \n",
    "            main_cast.append(whole_name_list[0].upper())            \n",
    "        elif 'King' in whole_name_list:\n",
    "            main_cast.append('KING GEORGE'.upper())        \n",
    "        else:\n",
    "            main_cast.append(whole_name_list[-1].upper())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAMILTON :  \t 4601\n",
      "BURR :  \t 2908\n",
      "ELIZA :  \t 1420\n",
      "ANGELICA :  \t 1081\n",
      "LAFAYETTE :  \t 574\n",
      "JEFFERSON :  \t 1180\n",
      "WASHINGTON :  \t 1271\n",
      "KING GEORGE :  \t 478\n",
      "LAURENS :  \t 684\n",
      "PHILIP :  \t 453\n",
      "PEGGY :  \t 108\n",
      "MARIA :  \t 113\n",
      "MULLIGAN :  \t 458\n",
      "MADISON :  \t 599\n"
     ]
    }
   ],
   "source": [
    "all_corpora = dict()\n",
    "for speaker in main_cast:\n",
    "     all_corpora[speaker] = []\n",
    "for song in haml:    \n",
    "    for line in song['lyrics']:\n",
    "        for speaker in line['speakers']:\n",
    "            if speaker in main_cast:\n",
    "                all_corpora[speaker].append(line['normalized'])\n",
    "\n",
    "word_c = []\n",
    "for speaker in main_cast:\n",
    "    all_corpora[speaker] = [item for sublist in all_corpora[speaker] for item in sublist]\n",
    "    print(speaker, ':  \\t', len(all_corpora[speaker]))\n",
    "    word_c.append(len(all_corpora[speaker]))\n",
    "    \n",
    "word_counts = pd.DataFrame(word_c,index=main_cast,columns=['#words'])\n",
    "word_counts.to_csv('../data/processed/word_counts.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rem_stopw_punct(string_):\n",
    "    # Remove common stopwords\n",
    "    stopwords = [\"a\", \"about\", \"above\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\",\n",
    "                  \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\",\n",
    "                  \"amoungst\", \"amount\", \"an\", \"and\", \"another\", \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\",\n",
    "                  \"anywhere\", \"are\", \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\", \"becomes\",\n",
    "                  \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\", \"besides\",\n",
    "                  \"between\", \"beyond\", \"bill\", \"both\", \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\",\n",
    "                  \"con\", \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\",\n",
    "                  \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\", \"elsewhere\", \"empty\", \"enough\",\n",
    "                  \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\",\n",
    "                  \"fifteen\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\",\n",
    "                  \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\",\n",
    "                  \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\",\n",
    "                  \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\",\n",
    "                  \"indeed\", \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\",\n",
    "                  \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\",\n",
    "                  \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\",\n",
    "                  \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\",\n",
    "                  \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\",\n",
    "                  \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\",\n",
    "                  \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"serious\",\n",
    "                  \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\",\n",
    "                  \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\",\n",
    "                  \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\",\n",
    "                  \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thickv\",\n",
    "                  \"thin\", \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\",\n",
    "                  \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\", \"twenty\", \"two\", \"un\", \"under\",\n",
    "                  \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\",\n",
    "                  \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\",\n",
    "                  \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\",\n",
    "                  \"why\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\",\n",
    "                  \"yourselves\", \"the\"]\n",
    "    pattern1 = re.compile(r'\\b(' + r'|'.join(stopwords) + r')\\b\\s*')\n",
    "    string_ = pattern1.sub('', string_)\n",
    "\n",
    "    # Remove punctuation, i.e. anything that's not a letter or space\n",
    "    pattern2 = re.compile(r'[^A-Za-z ]')\n",
    "    string_ = pattern2.sub(' ', string_).strip()\n",
    "\n",
    "    # Also, reduce any sequence of 2+ white spaces to a single one, while also stripping trailing blanks\n",
    "    pattern3 = re.compile(r'  +')\n",
    "    string_ = pattern3.sub(' ', string_).strip()\n",
    "    \n",
    "    return string_.translate(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use other bool sim formula?\n",
    "def bool_sim(st1, st2, df):\n",
    "    return (int(df[[st1]].multiply(df[st2], axis=\"index\").sum()))/(int(df[[st1]].sum()) * int(df[[st2]].sum()))\n",
    "#return float((df[[st1]].multiply(df[st2], axis=\"index\")).sum()) / (int(df[[st1]].sum()) * int(df[[st2]].sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### STOPPED HERE\n",
    "# Have to split it up again, right?\n",
    "for speaker in all_corpora:\n",
    "    all_corpora[speaker] = rem_stopw_punct(str(' '.join(all_corpora[speaker])).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get vocabulary\n",
    "corp_flat = []\n",
    "for speaker in all_corpora:\n",
    "    corp_flat.append(all_corpora[speaker].split(' '))\n",
    "corp_flat = [item for sublist in corp_flat for item in sublist]\n",
    "voc = set(corp_flat)\n",
    "\n",
    "# Create term-document matrix\n",
    "tdm = pd.DataFrame(0,index=voc,columns=main_cast)\n",
    "for speaker in all_corpora:\n",
    "    tokens = all_corpora[speaker].split(' ')\n",
    "    for word in tokens:\n",
    "        tdm.at[word,speaker] = 1\n",
    "tdm.to_csv('../data/processed/tdm.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool_sim(main_cast[0], main_cast[2], tdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-351-9e213b739114>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[0mdsm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmain_cast\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmain_cast\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcast2_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[0mdsm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmain_cast\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmain_cast\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcast2_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m             \u001b[0mdsm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmain_cast\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcast2_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmain_cast\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m             \u001b[0mbool_sim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain_cast\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmain_cast\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcast2_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtdm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mcounter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mdsm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../data/processed/dsm.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-344-457f1cb80868>\u001b[0m in \u001b[0;36mbool_sim\u001b[1;34m(st1, st2, df)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mbool_sim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mst1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mst2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mst1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mst2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"index\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mst1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mst2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/opt/anaconda3/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1984\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1985\u001b[0m             \u001b[1;31m# either boolean or fancy integer index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1986\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1987\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1988\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda3/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2028\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2029\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2030\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2031\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2032\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda3/lib/python3.5/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[1;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[0;32m   1198\u001b[0m                     \u001b[1;31m# unique index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1199\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1200\u001b[1;33m                         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjarr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1202\u001b[0m                     \u001b[1;31m# non-unique (dups)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda3/lib/python3.5/site-packages/pandas/indexes/base.py\u001b[0m in \u001b[0;36mget_indexer\u001b[1;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[0;32m   2026\u001b[0m                                  'backfill or nearest reindexing')\n\u001b[0;32m   2027\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2028\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2029\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2030\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_platform_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_indexer (pandas/index.c:5888)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.lookup (pandas/hashtable.c:13317)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "# Do boolean, td-idf similarity\n",
    "\n",
    "# Create document similarity matrix\n",
    "dsm = pd.DataFrame(0,index=main_cast,columns=main_cast)\n",
    "counter = 0\n",
    "for cast1 in main_cast:\n",
    "    for cast2_index in range(counter,len(main_cast)):\n",
    "        if (main_cast == main_cast[cast2_index]):\n",
    "            dsm.loc[main_cast, main_cast[cast2_index]] = 1\n",
    "        else:\n",
    "            dsm.loc[main_cast, main_cast[cast2_index]] = \\\n",
    "            dsm.loc[main_cast[cast2_index], main_cast] = \\\n",
    "            bool_sim(main_cast, main_cast[cast2_index], tdm)\n",
    "    counter += 1\n",
    "dsm.to_csv('../data/processed/dsm.csv', sep=' ')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
