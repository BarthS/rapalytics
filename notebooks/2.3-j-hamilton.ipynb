{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json, operator, os, re, requests, string, pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from pandas.io.json import json_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../data/processed/hamilton_data.json', 'r') as f:\n",
    "    haml = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "act#\n",
      "track#\n",
      "track\n",
      "lyrics\n"
     ]
    }
   ],
   "source": [
    "for key in haml[0]:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for song in haml:\n",
    "    song['act#'] = int(song['act#'])\n",
    "    song['track#'] = int(song['track#'])\n",
    "haml.sort(key=operator.itemgetter('act#','track#'))\n",
    "#for song in haml:\n",
    "#    print('Act:', song['act#'], 'Track#:', song['track#'], 'Track:', song['track'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "basestats = json_normalize(haml)[['act#', 'track#','track','lyrics']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "line_length, word_length = [], []\n",
    "for song in haml:    \n",
    "    line_length.append(len(song['lyrics']))\n",
    "    words_in_song = 0\n",
    "    for line in song['lyrics']:\n",
    "        words_in_song += len(line['tokenized'])\n",
    "    word_length.append(words_in_song)    \n",
    "basestats['#lines'] = line_length\n",
    "basestats['#words'] = word_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = requests.get('https://en.wikipedia.org/wiki/Hamilton_(album)')\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "tbl1, tbl2 = soup.find_all('table')[2], soup.find_all('table')[3]\n",
    "rows1, rows2 = tbl1.find_all('tr'), tbl2.find_all('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "song_length = []\n",
    "for row in rows1[:-1]:  # Exclude last line, cause it's just a summary\n",
    "    if len(row.find_all('td'))==0:\n",
    "        continue  # Exclude empty rows\n",
    "    row_compl = row.find_all('td')  # Find all cells\n",
    "    song_length.append(row_compl[3].text)  # Get row giving the length of song\n",
    "for row in rows2[:-1]:  # Exclude last line, cause it's just a summary\n",
    "    if len(row.find_all('td'))==0:\n",
    "        continue  # Exclude empty rows\n",
    "    row_compl = row.find_all('td')  # Find all cells\n",
    "    song_length.append(row_compl[3].text)  # Get row giving the length of song\n",
    "basestats['len_seconds'] = [int(m[0])*60+int(m[2:4]) for m in song_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_speakers = []\n",
    "speakers_song_ls = []\n",
    "for song in haml:\n",
    "    speakers_song = []\n",
    "    for line in song['lyrics']:\n",
    "        all_speakers.append(line['speakers'])\n",
    "        speakers_song.append(line['speakers'])\n",
    "    #speakers_song_ls.append(set(speakers_song))\n",
    "    speakers_song_ls.append(set([item for sublist in speakers_song for item in sublist]))\n",
    "    #print(set([item for sublist in speakers_song for item in sublist]))\n",
    "all_speakers = set([item for sublist in all_speakers for item in sublist])\n",
    "basestats['speakers'] = speakers_song_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_speakers = []\n",
    "for spkls in speakers_song_ls:\n",
    "    num_speakers.append(len(spkls))\n",
    "basestats['#speakers'] = num_speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "basestats['#words/sec'] = basestats['#words']/basestats['len_seconds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "basestats.drop('lyrics', axis=1, inplace=False).to_csv('../data/processed/basestats.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = requests.get('https://en.wikipedia.org/wiki/Hamilton_(musical)')\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "cast = soup.find_all('table')[2]\n",
    "castr = cast.find_all('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "main_cast = []\n",
    "for row in castr:\n",
    "    if len(row.find_all('th'))==0:\n",
    "        continue  # Exclude empty rows\n",
    "    row_compl = row.find_all('th')  # Find all cells\n",
    "    cur_speaker_row = row_compl[0].text.split(' / ')\n",
    "    for whole_name in cur_speaker_row:\n",
    "        whole_name_list = whole_name.split(' ')\n",
    "        if 'Character' in whole_name_list:\n",
    "            continue\n",
    "        # Women, children, and kings are referred to by first name\n",
    "        elif 'Schuyler' in whole_name_list:\n",
    "            main_cast.append(whole_name_list[0].upper())\n",
    "        elif 'Reynolds' in whole_name_list:          \n",
    "            main_cast.append(whole_name_list[0].upper())\n",
    "        elif 'Philip' in whole_name_list:          \n",
    "            main_cast.append(whole_name_list[0].upper())            \n",
    "        elif 'King' in whole_name_list:\n",
    "            main_cast.append(whole_name_list[1].upper())        \n",
    "        else:\n",
    "            main_cast.append(whole_name_list[-1].upper())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAMILTON :  4601\n",
      "BURR :  2908\n",
      "ELIZA :  1420\n",
      "ANGELICA :  1081\n",
      "LAFAYETTE :  574\n",
      "JEFFERSON :  1180\n",
      "WASHINGTON :  1271\n",
      "GEORGE :  527\n",
      "PEGGY :  108\n",
      "MARIA :  113\n",
      "LAURENS :  684\n",
      "PHILIP :  453\n",
      "MULLIGAN :  458\n",
      "MADISON :  599\n"
     ]
    }
   ],
   "source": [
    "all_corpora = dict()\n",
    "for speaker in main_cast:\n",
    "     all_corpora[speaker] = []\n",
    "for song in haml:    \n",
    "    for line in song['lyrics']:\n",
    "        for speaker in line['speakers']:\n",
    "            if speaker in main_cast:\n",
    "                all_corpora[speaker].append(line['normalized'])\n",
    "\n",
    "word_c = []\n",
    "for speaker in main_cast:\n",
    "    all_corpora[speaker] = [item for sublist in all_corpora[speaker] for item in sublist]\n",
    "    print(speaker, ': ', len(all_corpora[speaker]))\n",
    "    word_c.append(len(all_corpora[speaker]))\n",
    "    \n",
    "word_counts = pd.DataFrame(word_c,index=main_cast,columns=['#words'])\n",
    "word_counts.to_csv('../data/processed/word_counts.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rem_stopw_punct(string_):\n",
    "    # Remove common stopwords\n",
    "    stopwords = [\"a\", \"about\", \"above\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\",\n",
    "                  \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\",\n",
    "                  \"amoungst\", \"amount\", \"an\", \"and\", \"another\", \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\",\n",
    "                  \"anywhere\", \"are\", \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\", \"becomes\",\n",
    "                  \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\", \"besides\",\n",
    "                  \"between\", \"beyond\", \"bill\", \"both\", \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\",\n",
    "                  \"con\", \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\",\n",
    "                  \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\", \"elsewhere\", \"empty\", \"enough\",\n",
    "                  \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\",\n",
    "                  \"fifteen\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\",\n",
    "                  \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\",\n",
    "                  \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\",\n",
    "                  \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"ie\", \"if\", \"in\", \"inc\",\n",
    "                  \"indeed\", \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\",\n",
    "                  \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\",\n",
    "                  \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\",\n",
    "                  \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\",\n",
    "                  \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\",\n",
    "                  \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\",\n",
    "                  \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"serious\",\n",
    "                  \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\",\n",
    "                  \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\",\n",
    "                  \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\",\n",
    "                  \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thickv\",\n",
    "                  \"thin\", \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\",\n",
    "                  \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\", \"twenty\", \"two\", \"un\", \"under\",\n",
    "                  \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\",\n",
    "                  \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\",\n",
    "                  \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\",\n",
    "                  \"why\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\",\n",
    "                  \"yourselves\", \"the\"]\n",
    "    pattern1 = re.compile(r'\\b(' + r'|'.join(stopwords) + r')\\b\\s*')\n",
    "    string_ = pattern1.sub('', string_)\n",
    "\n",
    "    # Remove punctuation, i.e. anything that's not a letter or space\n",
    "    pattern2 = re.compile(r'[^A-Za-z ]')\n",
    "    string_ = pattern2.sub(' ', string_).strip()\n",
    "\n",
    "    # Also, reduce any sequence of 2+ white spaces to a single one, while also stripping trailing blanks\n",
    "    pattern3 = re.compile(r'  +')\n",
    "    string_ = pattern3.sub(' ', string_).strip()\n",
    "    \n",
    "    return string_.translate(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bool_sim(st1, st2, df):\n",
    "    return (int(df[[st1]].multiply(df[st2], axis=\"index\").sum()))/(int(df[[st1]].sum()) * int(df[[st2]].sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for speaker in all_corpora:\n",
    "    all_corpora[speaker] = rem_stopw_punct(str(' '.join(all_corpora[speaker])).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get vocabulary\n",
    "corp_flat = []\n",
    "for speaker in all_corpora:\n",
    "    corp_flat.append(all_corpora[speaker].split(' '))\n",
    "corp_flat = [item for sublist in corp_flat for item in sublist]\n",
    "voc = set(corp_flat)\n",
    "\n",
    "# Create term-document matrix\n",
    "tdm = pd.DataFrame(0,index=voc,columns=main_cast)\n",
    "for speaker in all_corpora:\n",
    "    tokens = all_corpora[speaker].split(' ')\n",
    "    for word in tokens:\n",
    "        tdm.at[word,speaker] = 1\n",
    "tdm.to_csv('../data/processed/tdm.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool_sim(main_cast[0], main_cast[2], tdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-351-9e213b739114>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[0mdsm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmain_cast\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmain_cast\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcast2_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[0mdsm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmain_cast\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmain_cast\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcast2_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m             \u001b[0mdsm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmain_cast\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcast2_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmain_cast\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m             \u001b[0mbool_sim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain_cast\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmain_cast\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcast2_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtdm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mcounter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mdsm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../data/processed/dsm.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-344-457f1cb80868>\u001b[0m in \u001b[0;36mbool_sim\u001b[1;34m(st1, st2, df)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mbool_sim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mst1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mst2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mst1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mst2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"index\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mst1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mst2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/opt/anaconda3/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1984\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1985\u001b[0m             \u001b[1;31m# either boolean or fancy integer index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1986\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1987\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1988\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda3/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2028\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2029\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2030\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2031\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2032\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda3/lib/python3.5/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[1;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[0;32m   1198\u001b[0m                     \u001b[1;31m# unique index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1199\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1200\u001b[1;33m                         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjarr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1202\u001b[0m                     \u001b[1;31m# non-unique (dups)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda3/lib/python3.5/site-packages/pandas/indexes/base.py\u001b[0m in \u001b[0;36mget_indexer\u001b[1;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[0;32m   2026\u001b[0m                                  'backfill or nearest reindexing')\n\u001b[0;32m   2027\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2028\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2029\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2030\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_platform_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_indexer (pandas/index.c:5888)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.lookup (pandas/hashtable.c:13317)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "# Do boolean, td-idf similarity\n",
    "### STOPPED HERE\n",
    "# Create document similarity matrix\n",
    "dsm = pd.DataFrame(0,index=main_cast,columns=main_cast)\n",
    "counter = 0\n",
    "for cast1 in main_cast:\n",
    "    for cast2_index in range(counter,len(main_cast)):\n",
    "        if (main_cast == main_cast[cast2_index]):\n",
    "            dsm.loc[main_cast, main_cast[cast2_index]] = 1\n",
    "        else:\n",
    "            dsm.loc[main_cast, main_cast[cast2_index]] = \\\n",
    "            dsm.loc[main_cast[cast2_index], main_cast] = \\\n",
    "            bool_sim(main_cast, main_cast[cast2_index], tdm)\n",
    "    counter += 1\n",
    "dsm.to_csv('../data/processed/dsm.csv', sep=' ')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
